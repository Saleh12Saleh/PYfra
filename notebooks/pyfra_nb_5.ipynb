{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aaa41b4",
   "metadata": {},
   "source": [
    "Notebook 5\n",
    "==============\n",
    "Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43e28c",
   "metadata": {},
   "source": [
    "# Outline\n",
    "The aim is to further investigate the models developed in the third notebook.\n",
    "We will\n",
    "1. Identify the relationship between amount of training data and model performance\n",
    "2. Compare the performance of our model with a naive approach of training on the un-stratified, imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e47bf8",
   "metadata": {},
   "source": [
    "# Import Modules, Data and Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fcb30b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "import pyfra\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65271718",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/df.p')\n",
    "n_rows_complete = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24fae0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether or not the data is up-to-date (file can't be tracked on github because of it's file size)\n",
    "pd.testing.assert_frame_equal(left=(pd.read_csv('../data/df_check_info.csv', index_col=0)), \\\n",
    "                         right=pyfra.df_testing_info(df),\\\n",
    "                         check_dtype=False, check_exact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18fc6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa48219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns='Severity',axis=1).select_dtypes(include=np.number).dropna(axis=1)\n",
    "target = df['Severity']\n",
    "data, target = rus.fit_resample(X=data, y=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed7f4bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6741\n",
       "2    6741\n",
       "3    6741\n",
       "4    6741\n",
       "Name: Severity, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae65fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are working on 26964 data points, which represent 10.7443% of the original data,\n"
     ]
    }
   ],
   "source": [
    "print(f'We are working on {len(target)} data points, which represent {len(target)/n_rows_complete*100:.04f}% of the original data,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3b6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns='Severity',axis=1).select_dtypes(include=np.number).dropna(axis=1)\n",
    "target = df['Severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f7b3a",
   "metadata": {},
   "source": [
    "# Relation between Amount of Training Data and Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dff1b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 1.2.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.2.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.2.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.2.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 1.2.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.2.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator StackingClassifier from version 1.2.1 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = load('../models/preprocessing_pipeline.joblib')\n",
    "svc = load('../models/svc.joblib')\n",
    "stacking_clf = load('../models/stacking_clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448fa068",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.verbose= 100\n",
    "stacking_clf.verbose = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a9d7a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a matrix to store the results\n",
    "result_metrics = pd.DataFrame(columns=['model', 'n_rows','f1', 'accuracy', 'recall'])\n",
    "result_metrics.index.name = 'id'\n",
    "result_metrics\n",
    "result_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed5fa9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of the data, because the whole dataset is too big for us to work with\n",
    "#df = df.sample(n=n_rows, random_state=23)\n",
    "from sklearn.utils import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90869f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to compute and store the results for the respective model\n",
    "from sklearn.utils import random\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "def store_metrics(model_label, model, n_rows, result_df):\n",
    "    id = result_df.shape[0]\n",
    "    result_df.loc[id, 'model_label'] = model_label\n",
    "    result_df.loc[id, 'model'] = model\n",
    "    result_df.loc[id, 'n_rows'] = n_rows\n",
    "    print(f'Splitting {n_rows} rows of data...')\n",
    "    sample_indices = random.sample_without_replacement(n_population=len(target), \n",
    "                                                       n_samples=n_rows)\n",
    "    data_sample = data.iloc[sample_indices]\n",
    "    target_sample = target.iloc[sample_indices]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_sample, \n",
    "                                                        target_sample, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=23, \n",
    "                                                        stratify=target_sample)\n",
    "    print(f'Preprocessing Data...')\n",
    "    X_train = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "    X_test = preprocessing_pipeline.transform(X_test)\n",
    "    print(f'Fitting {model_label}...')\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f'Predicting...')\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Computing scores...')\n",
    "    result_df.loc[id, 'f1'] = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    result_df.loc[id, 'accuracy'] = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    result_df.loc[id, 'recall'] = recall_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a692bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting 500 rows of data...\n",
      "Preprocessing Data...\n",
      "Fitting stacking...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 370 features, but LogisticRegression is expecting 60 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_rows \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1_000\u001b[39m, \u001b[38;5;241m2_000\u001b[39m, \u001b[38;5;241m5_000\u001b[39m, \u001b[38;5;241m10_000\u001b[39m, \u001b[38;5;241m20_000\u001b[39m]:\n\u001b[1;32m----> 2\u001b[0m     result_metrics \u001b[38;5;241m=\u001b[39m store_metrics(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstacking\u001b[39m\u001b[38;5;124m'\u001b[39m, stacking_clf, n_rows, result_metrics)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_metrics)\n",
      "Cell \u001b[1;32mIn [15], line 23\u001b[0m, in \u001b[0;36mstore_metrics\u001b[1;34m(model_label, model, n_rows, result_df)\u001b[0m\n\u001b[0;32m     21\u001b[0m X_test \u001b[38;5;241m=\u001b[39m preprocessing_pipeline\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicting...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:584\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le \u001b[38;5;241m=\u001b[39m LabelEncoder()\u001b[38;5;241m.\u001b[39mfit(y)\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m--> 584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_le\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:214\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method_name(name, est, meth)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(names, all_estimators, stack_method)\n\u001b[0;32m    210\u001b[0m ]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# Generate predictions from prefit models\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(estimator, predict_method)(X)\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m estimator, predict_method \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     ]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# To train the meta-classifier using the most data as possible, we use\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# a cross-validation to obtain the output of the stacked estimators.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# To ensure that the data provided to each estimator are the same,\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# we need to set the random state of the cv if there is one and we\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;66;03m# need to take a copy.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     cv \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y\u001b[38;5;241m=\u001b[39my, classifier\u001b[38;5;241m=\u001b[39mis_classifier(\u001b[38;5;28mself\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py:215\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method_name(name, est, meth)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(names, all_estimators, stack_method)\n\u001b[0;32m    210\u001b[0m ]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# Generate predictions from prefit models\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m estimator, predict_method \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     ]\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# To train the meta-classifier using the most data as possible, we use\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# a cross-validation to obtain the output of the stacked estimators.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# To ensure that the data provided to each estimator are the same,\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# we need to set the random state of the cv if there is one and we\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;66;03m# need to take a copy.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     cv \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y\u001b[38;5;241m=\u001b[39my, classifier\u001b[38;5;241m=\u001b[39mis_classifier(\u001b[38;5;28mself\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1311\u001b[0m, in \u001b[0;36mLogisticRegression.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1306\u001b[0m ovr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1308\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ovr:\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_proba_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n",
      "File \u001b[1;32mc:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:461\u001b[0m, in \u001b[0;36mLinearClassifierMixin._predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict_proba_lr\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    455\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Probability estimation for OvR logistic regression.\u001b[39;00m\n\u001b[0;32m    456\u001b[0m \n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03m    Positive class probabilities are computed as\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    1. / (1. + np.exp(-self.decision_function(X)));\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m    multiclass is handled by normalizing that over all classes.\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m     expit(prob, out\u001b[38;5;241m=\u001b[39mprob)\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prob\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:429\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;124;03mPredict confidence scores for samples.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    this class would be predicted.\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    427\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 429\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Louna\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 370 features, but LogisticRegression is expecting 60 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "for n_rows in [500, 1_000, 2_000, 5_000, 10_000, 20_000]:\n",
    "    result_metrics = store_metrics('stacking', stacking_clf, n_rows, result_metrics)\n",
    "print(result_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_metrics"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-LanguageId",
   "notebook_metadata_filter": "-kernelspec"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
